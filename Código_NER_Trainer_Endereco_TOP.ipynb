{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac #  wrapper over argparse\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from tqdm import tqdm, tqdm_notebook # loading bar \n",
    "import pandas as pd\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para substituir acentos\n",
    "\n",
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): # unicode is a default on python 3 \n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    \n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrindo e gerando Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entidade logradouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo dados crus de endereço\n",
    "\n",
    "dataset = pd.read_csv(\"201906AGENCIAS.CSV\")\n",
    "dset = dataset.iloc[:-2,4:10].values  # tipo nparray object\n",
    "pd_dset = pd.DataFrame(dset) # caso queira visualizar\n",
    "\n",
    "pd_dset['Join'] = pd_dset[pd_dset.columns[0:]].apply(\n",
    "    lambda x: ';'.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "pd_dset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando endereço completo do DataFrame\n",
    "dset = np.array(pd_dset)\n",
    "end_lista = []\n",
    "\n",
    "for i in range(len(dset)):\n",
    "    str_raw = dset[i][6]\n",
    "    str_tratada = re.sub(r'[ ]{2,}', \"\",str_raw) # Tirando espaços excedentes no final do endereço\n",
    "    str_tratada = re.sub(r'[;]{1,}', \"; \", str_tratada) # Para complementos vazios, para não ter 2 \";\"\n",
    "    str_tratada = str_tratada.lower()\n",
    "    str_tratada = str_tratada.replace(\",\", \" \")\n",
    "    str_tratada = str_tratada.replace(\".\", \" \")\n",
    "    str_tratada = strip_accents(str_tratada)\n",
    "    str_tratada = re.sub(r'[ ]{2,}', \" \", str_tratada)\n",
    "\n",
    "    end_lista.append(str_tratada)\n",
    "\n",
    "end_lista[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lógica para extrair posição do logradouro:\n",
    "# Dividir a String inteira por \";\", pegar o len do primeiro split\n",
    "# len de LOGRA é de 0 até len do primeiro split\n",
    "\n",
    "print(end_lista[0])\n",
    "split = end_lista[0].split(\";\")\n",
    "print(split)\n",
    "print(split[0])\n",
    "print(len(split[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo iob com entidade apenas de logradouro e fazendo mais tratamentos de texto\n",
    "\n",
    "iob = []\n",
    "\n",
    "for i in range(len(end_lista)):\n",
    "    split = end_lista[i].split(\";\")\n",
    "    len_logra = len(split[0])\n",
    "    iob_dict = {\"entities\": [(0, len_logra-1,'LOGRA')]}\n",
    "    end_lista[i] = end_lista[i].replace(\";\", \" \")\n",
    "    end_lista[i] = end_lista[i].replace(\"  \", \" \")\n",
    "    end_lista[i] = end_lista[i].replace(\",\", \" \")\n",
    "    end_lista[i] = re.sub(r'^[ ]+', \"\", end_lista[i])\n",
    "    \n",
    "    tupla = (end_lista[i], iob_dict)\n",
    "    iob.append(tupla)\n",
    "\n",
    "FULL_DATA = iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FULL_DATA[0])\n",
    "print(FULL_DATA[1])\n",
    "print(FULL_DATA[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da base de teste e treinamento\n",
    "\n",
    "n_test= 0.1 # Porcentagem para base de teste\n",
    "test_n = round(len(FULL_DATA) * n_test)\n",
    "\n",
    "# Divisao em Train Test Val\n",
    "\n",
    "def gerador_bases(dataset, n):\n",
    "    indices_random = random.sample(range(0,len(dataset)-1), n)\n",
    "    base_teste_n = []\n",
    "    base_treinamento_n = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        base_teste_n.append(dataset[indices_random[i]])\n",
    "\n",
    "    for j in range(len(dataset)):\n",
    "        if(j not in indices_random):\n",
    "            base_treinamento_n.append(dataset[j])\n",
    "            \n",
    "    return base_teste_n, base_treinamento_n\n",
    "\n",
    "\n",
    "base_teste, base_treinamento = gerador_bases(FULL_DATA, test_n)\n",
    "\n",
    "random.shuffle(base_treinamento)\n",
    "random.shuffle(base_teste)\n",
    "\n",
    "print(\"Treinamento: \" + str(len(base_treinamento)), \"\\nTeste: \" + str(len(base_teste)), \"\\nTotal: \" + str(len(FULL_DATA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(base_treinamento[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our variables\n",
    "\n",
    "model = None\n",
    "output_dir=Path(\".\")\n",
    "n_iter= 100 # número de épocas\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline and entity recognizer.\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spacy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    lan = 'pt'\n",
    "    nlp = spacy.blank(lan)  # create blank Language class\n",
    "    print(\"Created blank '%s' model\" % lan)\n",
    "    \n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "    print('Added new NER')\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "    print('Got an old NER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Épocas: \", n_iter)\n",
    "print()\n",
    "\n",
    "# add labels\n",
    "for _, annotations in base_treinamento:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(base_treinamento)\n",
    "        losses = {}\n",
    "        batches = minibatch(base_treinamento, size=batch_size)\n",
    "        \n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            try:\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "            except:\n",
    "                pass     \n",
    "        \n",
    "        print(itn+1, ' Losses', losses)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com base de treinamento\n",
    "for text, _ in base_teste:\n",
    "    doc = nlp(text)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tratamento de texto de entrada\n",
    "\n",
    "def regex(str_):\n",
    "    ph_tratada = str_.replace(\"\\\"\", \"\")\n",
    "    ph_tratada = ph_tratada.replace(\"-\", \" \")\n",
    "    ph_tratada = ph_tratada.replace(\"|\", \"\")\n",
    "    ph_tratada = ph_tratada.replace(\",\", \"\")\n",
    "    ph_tratada = ph_tratada.replace(\":\", \"\")\n",
    "    ph_tratada = ph_tratada.replace(\".\", \"\")\n",
    "    ph_tratada = ph_tratada.replace(\"  \", \"\")\n",
    "    ph_tratada = ph_tratada.lower()\n",
    "    ph_tratada = re.sub(r'[\\n]+', \" \", ph_tratada)\n",
    "    ph_tratada = re.sub(r'^[ ]+', \"\", ph_tratada)\n",
    "\n",
    "    ph_tratada = strip_accents(ph_tratada\n",
    "                              )    \n",
    "    return ph_tratada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com texto IPTU\n",
    "\n",
    "ph =\"\"\"\n",
    "        \"PREFEITURA DE Certidão de Dados Cadastrais do Imóvel - IPTU 2017\\n\\nSÃO PAULO\\n\\nFAZENDA\",\n",
    "        \"Cadastro do Imóvel: 067.061.0048-1\",\n",
    "        \"| Local do Imóvel:\",\n",
    "        \"| AVJACANA, 764- AP 104 E VG\",\n",
    "        \"BLOCO 1 UP LIFE SAO PAULO CEP 02273-001\\nImóvel localizado na 22 Subdivisão da Zona Urbana\",\n",
    "        \"| Endereço para entrega da notificação:\",\n",
    "        \"| AVJACANA, 764- AP 104 E VG\",\n",
    "        \"| BLOCO1 UPLIFE SAO PAULO CEP 02273-001\",\n",
    "        \"| Contribuinte(s):\",\n",
    "        \"| AGUA DAS FLORES EMPREENDIMENTOS IMOB LTDA\",\n",
    "        \"| Dados cadastrais do terreno:\",\n",
    "        \"| Área incorporada (m?): 1.669 Testada (m): 0,00\",\n",
    "        \"| Área não incorporada (m?): 0 Fração ideal: 0,0024\",\n",
    "        \"| Área total (m?): 7.669\",\n",
    "        \"| Dados cadastrais da construção:\",\n",
    "        \"| Área construída (m?):\\n\\n88\\n\\nPadrão da construção:\\n\\n2-C\",\n",
    "        \"Área ocupada pela construção (m?): 5.134 Uso: residência\",\n",
    "        \"| Ano da construção corrigido: 2016\",\n",
    "        \"| Valores de m? (R$):\",\n",
    "        \"| - de terreno: 700,00\",\n",
    "        \"| - da construção: 1.508,00\",\n",
    "        \"| Valores para fins de cálculo do IPTU (R$):\",\n",
    "        \"| - da área incorporada: 23.675,00\",\n",
    "        \"- da área não incorporada: 0,00\",\n",
    "        \"| - da construção: 131.377,00\",\n",
    "        \"|| Base de cálculo do IPTU: 155.052,00\",\n",
    "        \"Ressalvado o direito da Fazenda Pública do Município de São Paulo atualizar os dados constantes do Cadastro\\nImobiliário Fiscal, apurados ou verificados a qualquer tempo, inclusive em relação ao exercício abrangido por\\nesta certidão, a Secretaria Municipal da Fazenda CERTIFICA que os dados cadastrais acima foram utilizados no\\nlançamento do Imposto Predial e Territorial Urbano do imóvel do exercício de 2017.\",\n",
    "        \"Certidão expedida via Internet - Portaria SF nº 008/2004, de 28/01/2004.\",\n",
    "        \"A autenticidade desta certidão poderá ser confirmada, até o dia 05/02/2018, em\",\n",
    "        \"http://www .prefeitura.sp.gov.br/cidade/secretarias/financas/servicos/certidoes/\",\n",
    "        \"| Data de Emissão: 07/11/2017\",\n",
    "        \"| Número do Documento: 2.2017.001599486-1\",\n",
    "        \"|| Solicitante: PAULO AFONSO DECICINO (CPF 892.407.408-30)\"\n",
    "    \"\"\"      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_tratada = regex(ph)\n",
    "print(ph_tratada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ph = nlp(ph_tratada)\n",
    "print('Entities', [(ent.text, ent.label_) for ent in doc_ph.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com texto qualquer\n",
    "\n",
    "phrase = regex(\"rua alameda gravatá quadra 301 conjunto 5 lote 08\")\n",
    "\n",
    "doc = nlp(phrase)\n",
    "print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
